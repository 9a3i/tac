{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping: récupération des archives du journal Le Soir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un répertoire pour stocker tous les articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = '../data/archives_lesoir'\n",
    "# Créer le dossier s'il n'existe pas\n",
    "if not os.path.exists(txt_path):\n",
    "    os.mkdir(txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les URLs de 100 articles contenant le mot \"linguistique\"\n",
    "\n",
    "Le journal [Le Soir](https://www.lesoir.be) met à disposition un moteur de recherche pour accéder à ses archives depuis 1989 (2 millions d'articles).\n",
    "Il est possible de faire des recherches ciblées et de récupérer les articles répondant à cette recherche.\n",
    "\n",
    "Essayons une recherche sur le thème \"linguistique\". Le journal renvoie 10 articles par page. On va donc récupérer les articles sur les 10 premières pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing page 0\n",
      "6 articles trouvés\n",
      "Parsing page 1\n",
      "10 articles trouvés\n",
      "Parsing page 2\n",
      "10 articles trouvés\n",
      "Parsing page 3\n",
      "10 articles trouvés\n",
      "Parsing page 4\n",
      "16 articles trouvés\n",
      "Parsing page 5\n",
      "20 articles trouvés\n",
      "Parsing page 6\n",
      "24 articles trouvés\n",
      "Parsing page 7\n",
      "28 articles trouvés\n",
      "Parsing page 8\n",
      "33 articles trouvés\n",
      "Parsing page 9\n",
      "33 articles trouvés\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "articles_urls = list()\n",
    "query = \"linguistique\"\n",
    "for i in range(0, 10):\n",
    "    print(f\"Parsing page {i}\")\n",
    "    url = f\"https://www.lesoir.be/archives/recherche?word={query}&sort=date%20desc&datefilter=anytime&start={i*10}\"\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    html = resp.text\n",
    "    pattern = r\"/art/[^\\?]*\"\n",
    "    for art_url in re.findall(pattern, html):\n",
    "        art_url = \"https://www.lesoir.be\" + art_url\n",
    "        if art_url not in articles_urls:\n",
    "            articles_urls.append(art_url)\n",
    "    i += 10\n",
    "    print(f\"{len(articles_urls)} articles trouvés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "['https://www.lesoir.be/art/d-20220121-GT3E0E', 'https://www.lesoir.be/art/d-20220710-GW6N52', 'https://www.lesoir.be/art/d-20220829-GWZURP', 'https://www.lesoir.be/art/d-20220828-GWZ79R', 'https://www.lesoir.be/art/d-20220826-GWXYRY', 'https://www.lesoir.be/art/d-20220818-GWU1KZ', 'https://www.lesoir.be/art/d-20220811-GWPVM1', 'https://www.lesoir.be/art/d-20220804-GWL6RJ', 'https://www.lesoir.be/art/d-20220729-GWHFEQ', 'https://www.lesoir.be/art/d-20220728-GWGZ0L']\n"
     ]
    }
   ],
   "source": [
    "# Impression du nombre d'urls récupérées\n",
    "print(len(articles_urls))\n",
    "# Impression des 10 premières URLs\n",
    "print(articles_urls[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Télécharger tous les articles et imprimer le titre, le chapô et la partie de l'article visible sans abonnement dans un fichier texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in articles_urls:\n",
    "    article_id = url.split(\"/\")[-1]\n",
    "    filename = os.path.join(txt_path, article_id + \".txt\")\n",
    "    if not os.path.exists(filename):\n",
    "        \n",
    "        resp = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        title = soup.find(\"h1\").get_text(separator=' ')\n",
    "        chapo = soup.find(\"r-article--chapo\").get_text(separator=' ')\n",
    "        content = soup.find(\"r-article--section\").get_text(separator=' ')\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(f\"{title}\\n\\n{chapo}\\n\\n{content}\")\n",
    "    else:\n",
    "        print(f\"{article_id}: déjà téléchargé\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifier que tous les articles ont été téléchargés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ce n'est pas le cas, vous pouvez relancer l'étape de téléchargement (elle ignorera les documents déjà téléchargés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 files found out of 33!\n"
     ]
    }
   ],
   "source": [
    "ok_count = 0\n",
    "for url in articles_urls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    downloads = os.listdir(txt_path)\n",
    "    if f\"{filename}.txt\" not in downloads:\n",
    "        print(f\"{filename} is missing!\")\n",
    "    else:\n",
    "        ok_count += 1\n",
    "print(f\"{ok_count} files found out of {len(articles_urls)}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour en savoir plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le web scraping avec Python: https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "- Tutoriel sur les expressions régulières: https://www.w3schools.com/python/python_regex.asp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv_tac': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a942b0119f0c2604d4302f32a2a6e790f63eb4c9b0c297be7a26bd56fa8e02c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
