{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping: récupération des archives du journal Le Soir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un répertoire pour stocker tous les articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = '../data/archives_lesoir'\n",
    "# Créer le dossier s'il n'existe pas\n",
    "if not os.path.exists(txt_path):\n",
    "    os.mkdir(txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les URLs de 100 articles contenant le mot \"linguistique\"\n",
    "\n",
    "Le journal [Le Soir](https://www.lesoir.be) met à disposition un moteur de recherche pour accéder à ses archives depuis 1989 (2 millions d'articles).\n",
    "Il est possible de faire des recherches ciblées et de récupérer les articles répondant à cette recherche.\n",
    "\n",
    "Essayons une recherche sur le thème \"linguistique\". Le journal renvoie 10 articles par page, mais nous nous focaliserons uniquement sur les articles payants. On va récupérer les URLs des articles sur les 100 premières pages, jusqu'à ce qu'on ait récupéré 100 urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "articles_urls = list()\n",
    "query = \"linguistique\"\n",
    "for i in range(0, 100):\n",
    "    print(f\"Parsing page {i}\")\n",
    "    url = f\"https://www.lesoir.be/archives/recherche?word={query}&sort=date%20desc&datefilter=anytime&start={i*10}\"\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    # Pattern pour extraire les urls des articles (/art/d-20220624-GVXEL9?)\n",
    "    pattern = r\"/art/[^\\?]*\"\n",
    "    for art_url in re.findall(pattern, html):\n",
    "        art_url = \"https://www.lesoir.be\" + art_url\n",
    "        # Vérification que l'url n'est pas déjà dans la liste\n",
    "        if art_url not in articles_urls:\n",
    "            articles_urls.append(art_url)\n",
    "    i += 10\n",
    "    if len(articles_urls) >= 100:\n",
    "        break\n",
    "    print(f\"{len(articles_urls)} articles trouvés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impression du nombre d'urls récupérées\n",
    "print(len(articles_urls))\n",
    "# Impression des 10 premières URLs\n",
    "print(articles_urls[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Télécharger tous les articles et imprimer le titre, le chapô et la partie de l'article visible sans abonnement dans un fichier texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in articles_urls:\n",
    "    # Récupération de l'id de l'article sur la base de son URL\n",
    "    article_id = url.split(\"/\")[-1]\n",
    "    # Création d'un nom de fichier et vérification qu'il n'existe pas déjà\n",
    "    filename = os.path.join(txt_path, article_id + \".txt\")\n",
    "    if not os.path.exists(filename):\n",
    "        # Téléchargement de l'article et parsing du HTML\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        # Récupération du titre, du chapô et du contenu visible\n",
    "        title = soup.find(\"h1\").get_text(separator=' ')\n",
    "        chapo = soup.find(\"r-article--chapo\").get_text(separator=' ')\n",
    "        content = soup.find(\"r-article--section\").get_text(separator=' ')\n",
    "        # Ecriture du fichier\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(f\"{title}\\n\\n{chapo}\\n\\n{content}\")\n",
    "    else:\n",
    "        print(f\"{article_id}: déjà téléchargé\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifier que tous les articles ont été téléchargés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ce n'est pas le cas, vous pouvez relancer l'étape de téléchargement (elle ignorera les documents déjà téléchargés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_count = 0\n",
    "for url in articles_urls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    downloads = os.listdir(txt_path)\n",
    "    if f\"{filename}.txt\" not in downloads:\n",
    "        print(f\"{filename} is missing!\")\n",
    "    else:\n",
    "        ok_count += 1\n",
    "print(f\"{ok_count} files found out of {len(articles_urls)}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour en savoir plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le web scraping avec Python: https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "- Tutoriel sur les expressions régulières: https://www.w3schools.com/python/python_regex.asp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv_tac': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a942b0119f0c2604d4302f32a2a6e790f63eb4c9b0c297be7a26bd56fa8e02c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
