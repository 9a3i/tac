{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Pair Programming\n",
    "\n",
    "Dans ce notebook nous allons demander à une intelligence artificielle de générer du code Python pour réaliser des tâches de traitement automatique de corpus.\n",
    "\n",
    "Avant de commencer, choisissez un outil comme [Bard](https://bard.google.com/u/2/chat) ou [ChatGPT](https://chat.openai.com/) et créez un compte.\n",
    "\n",
    "Vous pouvez ensuite demander à l'outil de créer du code. Avant de commencer, n'hésitez pas à lire [cet article](https://exocoding.com/ai-code-generation/) qui détaille les **bonnes pratiques** pour créer des _prompts_ efficaces dans le cadre de la génération de code par l'intelligence artificielle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Algorithme simple en Python\n",
    "\n",
    "Demandez à l'IA de générer un code python qui lance un décompte du réveillon du Nouvel An. Le code doit imprimer les nombres de 10 à 0 avec un intervalle d'une seconde, puis imprimer \"Bonne année\" à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "Bonne année !\n"
     ]
    }
   ],
   "source": [
    "# Votre code ici\n",
    "import time\n",
    "\n",
    "for i in range(10, -1, -1):\n",
    "    print(i)\n",
    "    if i > 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Bonne année !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Détection du sujet de la phrase\n",
    "\n",
    "Demandez à l'IA d'extraire le sujet dans une phrase.\n",
    "Demandez ensuite de générer le code Python qui réalise cette tâche et testez le ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sujet extrait : La Réunion\n"
     ]
    }
   ],
   "source": [
    "# Votre code ici\n",
    "def extraire_sujet(phrase: str) -> str:\n",
    "    # On suppose que le verbe est \"est\" et on prend tout ce qui est avant\n",
    "    separateur = \" est\"\n",
    "    if separateur in phrase:\n",
    "        sujet = phrase.split(separateur)[0].strip()\n",
    "        return sujet\n",
    "    else:\n",
    "        # Si \"est\" n'est pas trouvé, on peut retourner la phrase entière\n",
    "        return phrase.strip()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "phrase = \"La Réunion est une très belle île\"\n",
    "sujet = extraire_sujet(phrase)\n",
    "print(\"Sujet extrait :\", sujet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entités nommmées liées à Wikidata\n",
    "\n",
    "Demandez à l'IA d'extraire les entités nommées d'un texte en français, et de les lier à un identifiant wikidata.\n",
    "Demandez ensuite à l'IA de générer le code Python pour réaliser cette tâche et testez le ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités nommées détectées :\n",
      "Ukraine -> LOC\n",
      "Kiev -> LOC\n",
      "Ukraine -> LOC\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacyfishing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ent.text, \u001b[33m\"\u001b[39m\u001b[33m->\u001b[39m\u001b[33m\"\u001b[39m, ent.label_)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacyfishing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EntityFishing\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Charger spaCy français\u001b[39;00m\n\u001b[32m     24\u001b[39m nlp = spacy.load(\u001b[33m\"\u001b[39m\u001b[33mfr_core_news_md\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacyfishing'"
     ]
    }
   ],
   "source": [
    "# Votre code ici\n",
    "import spacy\n",
    "\n",
    "# 1. Installer le modèle une fois dans le terminal :\n",
    "#    python -m spacy download fr_core_news_sm\n",
    "\n",
    "# 2. Puis lancer ce script\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "texte = (\"La dernière version du plan de paix sur l’Ukraine est jugée \"\n",
    "         \"« significativement meilleure » pour Kiev, permettant notamment \"\n",
    "         \"à l’Ukraine de conserver une armée de 800.000 hommes.\")\n",
    "\n",
    "doc = nlp(texte)\n",
    "\n",
    "print(\"Entités nommées détectées :\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)\n",
    "\n",
    "import spacy\n",
    "from spacyfishing import EntityFishing\n",
    "\n",
    "# Charger spaCy français\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "# Ajouter le composant entity-fishing au pipeline\n",
    "nlp.add_pipe(\"entityfishing\", config={\n",
    "    \"url\": \"https://cloud.science-miner.com/nerd\"  # ou ton serveur entity-fishing\n",
    "})\n",
    "\n",
    "texte = (\"La dernière version du plan de paix sur l’Ukraine est jugée \"\n",
    "         \"« significativement meilleure » pour Kiev, permettant notamment \"\n",
    "         \"à l’Ukraine de conserver une armée de 800.000 hommes.\")\n",
    "\n",
    "doc = nlp(texte)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    # ent._.kb_qid contient normalement l’ID Wikidata (ex: 'Q212' pour l’Ukraine)\n",
    "    print(ent.text, \"->\", ent._.kb_qid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. A vous de jouer\n",
    "\n",
    "Pensez à une analyse que vous voudriez faire sur un texte. Demandez à l'IA de générer un code python qui réalise cette analyse et testez le ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\durie\\Documents\\tac\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\durie\\Documents\\tac\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\durie\\.cache\\huggingface\\hub\\models--Davlan--xlm-roberta-base-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Pipeline de reconnaissance d'entités nommées (NER)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ner_pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDavlan/xlm-roberta-base-ner-hrl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# modèle NER multilingue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimple\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# regroupe les tokens d'une même entité\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2. Pipeline d'analyse des sentiments (multi-langues)\u001b[39;00m\n\u001b[32m     12\u001b[39m sentiment_pipe = pipeline(\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msentiment-analysis\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mtabularisai/multilingual-sentiment-analysis\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\durie\\Documents\\tac\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1017\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1012\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1013\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mYou cannot use both `pipeline(... dtype=..., model_kwargs=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:...})` as those\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1014\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m arguments might conflict, use only one.)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1015\u001b[39m         )\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m, dtype):\n\u001b[32m   1018\u001b[39m         dtype = \u001b[38;5;28mgetattr\u001b[39m(torch, dtype)\n\u001b[32m   1019\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m] = dtype\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Votre code ici\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Pipeline de reconnaissance d'entités nommées (NER)\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"Davlan/xlm-roberta-base-ner-hrl\",   # modèle NER multilingue\n",
    "    aggregation_strategy=\"simple\"             # regroupe les tokens d'une même entité\n",
    ")\n",
    "\n",
    "# 2. Pipeline d'analyse des sentiments (multi-langues)\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"tabularisai/multilingual-sentiment-analysis\"\n",
    ")\n",
    "\n",
    "texte = (\n",
    "    \"C’est toujours le même cirque. Chaque annonce d’un nouvel album de Kanye West \"\n",
    "    \"charrie son lot de controverses, d’indignations, d’outrances et de sorties \"\n",
    "    \"médiatiques fracassantes. Et tout le monde de tomber à chaque fois dans le \"\n",
    "    \"panneau : Kanye West, artiste total ou escroc ? Génie ou demeuré ? Prophète \"\n",
    "    \"ou pervers narcissique ?\"\n",
    ")\n",
    "\n",
    "# 1) Extraction des entités nommées\n",
    "ents = ner_pipe(texte)\n",
    "\n",
    "# 2) Pour chaque entité, on analyse le sentiment du texte qui la contient\n",
    "resultats = []\n",
    "for ent in ents:\n",
    "    span_text = ent[\"word\"]\n",
    "    # on passe toute la phrase au modèle de sentiments (ici : le texte complet pour simplifier)\n",
    "    sentiment = sentiment_pipe(texte)[0]\n",
    "    resultats.append({\n",
    "        \"entite\": span_text,\n",
    "        \"type\": ent[\"entity_group\"],\n",
    "        \"sentiment_label\": sentiment[\"label\"],\n",
    "        \"sentiment_score\": float(sentiment[\"score\"])\n",
    "    })\n",
    "\n",
    "# Affichage des résultats\n",
    "for r in resultats:\n",
    "    print(\n",
    "        f\"Entité: {r['entite']:<25} \"\n",
    "        f\"Type: {r['type']:<8} \"\n",
    "        f\"Sentiment: {r['sentiment_label']:<10} \"\n",
    "        f\"Score: {r['sentiment_score']:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pour aller plus loin...\n",
    "\n",
    "En tant qu'étudiant ULB vous avez accès gratuitement au [Github student pack](https://education.github.com/pack). Ce pack vous donne permet d'utiliser [Github copilot](https://github.com/features/copilot), un outil d'auto-complétion de code grâce à l'intelligence artificielle. Ceci peut être très utile si vous voulez réaliser des tâches complexes sans être un expert en python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
